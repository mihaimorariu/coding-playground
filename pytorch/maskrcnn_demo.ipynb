{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import sys\n",
    "import utils\n",
    "\n",
    "sys.path.append('/home/mmorariu/dev/playground/src/pytorch')\n",
    "\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = 2\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 2\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/60]  eta: 0:02:45  lr: 0.000090  loss: 4.7946 (4.7946)  loss_classifier: 0.6044 (0.6044)  loss_box_reg: 0.3444 (0.3444)  loss_mask: 3.8130 (3.8130)  loss_objectness: 0.0170 (0.0170)  loss_rpn_box_reg: 0.0158 (0.0158)  time: 2.7645  data: 0.2433  max mem: 2593\n",
      "Epoch: [0]  [10/60]  eta: 0:01:21  lr: 0.000936  loss: 1.7091 (2.7115)  loss_classifier: 0.4355 (0.4095)  loss_box_reg: 0.2175 (0.2302)  loss_mask: 1.0013 (2.0336)  loss_objectness: 0.0174 (0.0225)  loss_rpn_box_reg: 0.0158 (0.0157)  time: 1.6266  data: 0.0251  max mem: 2883\n",
      "Epoch: [0]  [20/60]  eta: 0:01:01  lr: 0.001783  loss: 0.9415 (1.7169)  loss_classifier: 0.2135 (0.2882)  loss_box_reg: 0.1598 (0.1838)  loss_mask: 0.4034 (1.2036)  loss_objectness: 0.0230 (0.0240)  loss_rpn_box_reg: 0.0185 (0.0173)  time: 1.4855  data: 0.0040  max mem: 2883\n",
      "Epoch: [0]  [30/60]  eta: 0:00:46  lr: 0.002629  loss: 0.5764 (1.3470)  loss_classifier: 0.1169 (0.2306)  loss_box_reg: 0.1256 (0.1791)  loss_mask: 0.2585 (0.9009)  loss_objectness: 0.0125 (0.0189)  loss_rpn_box_reg: 0.0155 (0.0175)  time: 1.4939  data: 0.0047  max mem: 2883\n",
      "Epoch: [0]  [40/60]  eta: 0:00:29  lr: 0.003476  loss: 0.4217 (1.1108)  loss_classifier: 0.0639 (0.1893)  loss_box_reg: 0.1054 (0.1617)  loss_mask: 0.2017 (0.7279)  loss_objectness: 0.0065 (0.0156)  loss_rpn_box_reg: 0.0110 (0.0164)  time: 1.4413  data: 0.0046  max mem: 2883\n",
      "Epoch: [0]  [50/60]  eta: 0:00:15  lr: 0.004323  loss: 0.3472 (0.9673)  loss_classifier: 0.0485 (0.1616)  loss_box_reg: 0.0903 (0.1478)  loss_mask: 0.1826 (0.6289)  loss_objectness: 0.0039 (0.0132)  loss_rpn_box_reg: 0.0110 (0.0157)  time: 1.4658  data: 0.0046  max mem: 2883\n",
      "Epoch: [0]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.3115 (0.8664)  loss_classifier: 0.0424 (0.1440)  loss_box_reg: 0.0729 (0.1358)  loss_mask: 0.1746 (0.5604)  loss_objectness: 0.0014 (0.0115)  loss_rpn_box_reg: 0.0100 (0.0147)  time: 1.4644  data: 0.0045  max mem: 2883\n",
      "Epoch: [0] Total time: 0:01:29 (1.4917 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:22  model_time: 0.3406 (0.3406)  evaluator_time: 0.0196 (0.0196)  time: 0.4502  data: 0.0883  max mem: 2883\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.2504 (0.2702)  evaluator_time: 0.0054 (0.0090)  time: 0.2714  data: 0.0025  max mem: 2883\n",
      "Test: Total time: 0:00:14 (0.2846 s / it)\n",
      "Averaged stats: model_time: 0.2504 (0.2702)  evaluator_time: 0.0054 (0.0090)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.663\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.818\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.517\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.670\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.290\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.724\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.724\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.710\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.725\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.716\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.893\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.501\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.725\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.305\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.759\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.740\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.762\n",
      "Epoch: [1]  [ 0/60]  eta: 0:01:15  lr: 0.005000  loss: 0.2345 (0.2345)  loss_classifier: 0.0235 (0.0235)  loss_box_reg: 0.0286 (0.0286)  loss_mask: 0.1773 (0.1773)  loss_objectness: 0.0007 (0.0007)  loss_rpn_box_reg: 0.0044 (0.0044)  time: 1.2643  data: 0.1247  max mem: 2883\n",
      "Epoch: [1]  [10/60]  eta: 0:01:11  lr: 0.005000  loss: 0.2345 (0.2761)  loss_classifier: 0.0265 (0.0432)  loss_box_reg: 0.0419 (0.0526)  loss_mask: 0.1532 (0.1659)  loss_objectness: 0.0013 (0.0023)  loss_rpn_box_reg: 0.0068 (0.0121)  time: 1.4348  data: 0.0146  max mem: 2883\n",
      "Epoch: [1]  [20/60]  eta: 0:01:00  lr: 0.005000  loss: 0.2532 (0.2733)  loss_classifier: 0.0424 (0.0457)  loss_box_reg: 0.0440 (0.0476)  loss_mask: 0.1495 (0.1628)  loss_objectness: 0.0014 (0.0024)  loss_rpn_box_reg: 0.0095 (0.0148)  time: 1.5247  data: 0.0041  max mem: 2889\n",
      "Epoch: [1]  [30/60]  eta: 0:00:45  lr: 0.005000  loss: 0.2411 (0.2616)  loss_classifier: 0.0378 (0.0435)  loss_box_reg: 0.0304 (0.0450)  loss_mask: 0.1399 (0.1574)  loss_objectness: 0.0009 (0.0023)  loss_rpn_box_reg: 0.0083 (0.0134)  time: 1.5651  data: 0.0045  max mem: 2889\n",
      "Epoch: [1]  [40/60]  eta: 0:00:30  lr: 0.005000  loss: 0.2056 (0.2528)  loss_classifier: 0.0347 (0.0416)  loss_box_reg: 0.0226 (0.0407)  loss_mask: 0.1494 (0.1562)  loss_objectness: 0.0009 (0.0021)  loss_rpn_box_reg: 0.0076 (0.0121)  time: 1.5419  data: 0.0045  max mem: 3068\n",
      "Epoch: [1]  [50/60]  eta: 0:00:15  lr: 0.005000  loss: 0.2056 (0.2441)  loss_classifier: 0.0319 (0.0397)  loss_box_reg: 0.0231 (0.0375)  loss_mask: 0.1382 (0.1530)  loss_objectness: 0.0010 (0.0021)  loss_rpn_box_reg: 0.0086 (0.0117)  time: 1.5268  data: 0.0045  max mem: 3068\n",
      "Epoch: [1]  [59/60]  eta: 0:00:01  lr: 0.005000  loss: 0.2058 (0.2422)  loss_classifier: 0.0319 (0.0395)  loss_box_reg: 0.0224 (0.0359)  loss_mask: 0.1311 (0.1530)  loss_objectness: 0.0009 (0.0022)  loss_rpn_box_reg: 0.0091 (0.0116)  time: 1.5197  data: 0.0045  max mem: 3068\n",
      "Epoch: [1] Total time: 0:01:31 (1.5254 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:23  model_time: 0.3538 (0.3538)  evaluator_time: 0.0101 (0.0101)  time: 0.4619  data: 0.0963  max mem: 3068\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.2468 (0.2652)  evaluator_time: 0.0034 (0.0054)  time: 0.2642  data: 0.0027  max mem: 3068\n",
      "Test: Total time: 0:00:13 (0.2765 s / it)\n",
      "Averaged stats: model_time: 0.2468 (0.2652)  evaluator_time: 0.0034 (0.0054)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.771\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.992\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.935\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.585\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.776\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.330\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.815\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.815\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.780\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.818\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.992\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.952\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.576\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.767\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.790\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.790\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.760\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.792\n",
      "That's it!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
